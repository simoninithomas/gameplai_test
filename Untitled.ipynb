{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: Le module spécifié est introuvable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-809af13f6ddb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mvizdoom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: Le module spécifié est introuvable."
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from vizdoom import *\n",
    "\n",
    "from random import choice\n",
    "from time import sleep, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HELPER.PY\n",
    "\"\"\"\n",
    "Preprocess part:\n",
    "    We don't need to turn to gray thanks to vizdoom : game.set_screen_format(ScreenFormat.GRAY8)\n",
    "\"\"\"\n",
    "def preprocess_frame(frame, turn_to_gray):\n",
    "    \n",
    "    # Get the state\n",
    "    state = frame[10:-10,30:-30]\n",
    "    \n",
    "    # Grayscale it (if needed)\n",
    "    if turn_to_gray == True:\n",
    "        frame = cv2.cvtColor(frame, cv2.color.RGB2GRAY)\n",
    "    \n",
    "    # Resize it\n",
    "    state = scipy.misc.imresize(state, [84,84])\n",
    "    \n",
    "    # Normalize it\n",
    "    state = np.reshape(state,[np.prod(state.shape)]) / 255.0\n",
    "    \n",
    "    return state # Return a [84, 84, 1] frame\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Config doom environement\n",
    "\"\"\"\n",
    "def config_doom(action_size):\n",
    "        #The Below code is related to setting up the Doom environment\n",
    "        game.set_doom_scenario_path(\"basic.wad\") #This corresponds to the simple task we will pose our agent\n",
    "        game.set_doom_map(\"map01\")\n",
    "        game.set_screen_resolution(ScreenResolution.RES_160X120)\n",
    "        game.set_screen_format(ScreenFormat.GRAY8)\n",
    "        game.set_render_hud(False)\n",
    "        game.set_render_crosshair(False)\n",
    "        game.set_render_weapon(True)\n",
    "        game.set_render_decals(False)\n",
    "        game.set_render_particles(False)\n",
    "        game.add_available_button(Button.MOVE_LEFT)\n",
    "        game.add_available_button(Button.MOVE_RIGHT)\n",
    "        game.add_available_button(Button.ATTACK)\n",
    "        game.add_available_game_variable(GameVariable.AMMO2)\n",
    "        game.add_available_game_variable(GameVariable.POSITION_X)\n",
    "        game.add_available_game_variable(GameVariable.POSITION_Y)\n",
    "        game.set_episode_timeout(300)\n",
    "        game.set_episode_start_time(10)\n",
    "        game.set_window_visible(False)\n",
    "        game.set_sound_enabled(False)\n",
    "        game.set_living_reward(-1)\n",
    "        game.set_mode(Mode.PLAYER)\n",
    "        game.init()\n",
    "        self.actions = self.actions = np.identity(action_size,dtype=bool).tolist()\n",
    "        #End Doom set-up\n",
    "        self.env = game\n",
    "        \n",
    "        return self.actions, self.env\n",
    "\n",
    "    # Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "# Discounting function used to calculate discounted returns.\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "#Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_episode_length = 300\n",
    "state_size = 7056 # Observations are greyscale frames of 84 * 84 * 1\n",
    "action_size = 3 # Agent can move Left, Right, or Fire\n",
    "gamma = .99 # discount rate for advantage estimation and reward discounting\n",
    "load_model = False\n",
    "model_path = './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AC Network: All the tensorflow operations to create the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"a3c.png\" alt=\"A3C Model\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really important <b> to initialise the weights and biases well </b>\n",
    "<ul>\n",
    "<li> By using \"Xavier initialization\" which chooses the variance of the distribution (either uniformly or normally distributed) to be 1/Nin where Nin is the number of neurons that are input to a given neuron in this layer. </li>\n",
    "\n",
    "<li> For FC is easy: the number of neurons input to a given layer = number of neurons in the previous layer </li>\n",
    "<li> So if the filter size of the current layer is [4,4] and there are 16 filters on the previous layer, then the number of inputs would be 4×4×16. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AC NETWORK\n",
    "class AC_Network:\n",
    "    def __init__(self, state_size, action_size, scope, trainer):\n",
    "        with tf.variable_scope(scope):\n",
    "            # State_size = 84*84 = 7056\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, state_size])\n",
    "\n",
    "            self.states = tf.reshape(self.inputs, [-1, 84, 84, 1])\n",
    "\n",
    "            # CNN for spatial dependencies\n",
    "            self.conv1 = tf.contrib.layers.conv2d(self.states,\n",
    "                                                 num_outputs = 16,\n",
    "                                                 kernel_size = [8,8],\n",
    "                                                 stride = [4,4],\n",
    "                                                 padding = \"VALID\",\n",
    "                                                 activation_fn = tf.nn.elu,\n",
    "                                                 name = \"conv1\")\n",
    "\n",
    "            self.conv2 = tf.contrib.layers.conv2d(self.conv1,\n",
    "                                                 num_outputs = 32,\n",
    "                                                 kernel_size = [4,4],\n",
    "                                                 stride = [2,2],\n",
    "                                                 padding = \"VALID\",\n",
    "                                                 activation_fn = tf.nn.elu,\n",
    "                                                 name = \"conv2\")\n",
    "\n",
    "            # TODO: See if good idea to add a conv, maxpool\n",
    "\n",
    "            # Flatten the network\n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv2)\n",
    "\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.flatten,\n",
    "                                                        num_outputs = 256,\n",
    "                                                        activation_fn = tf.nn.elu,\n",
    "                                                        name = \"fc1\")\n",
    "\n",
    "            # LSTM for temporal dependencies\n",
    "            lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(256,state_is_tuple=True)\n",
    "            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "            self.state_init = [c_init, h_init]\n",
    "\n",
    "            c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "            h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "            self.state_in = (c_in, h_in)\n",
    "\n",
    "            rnn_in = tf.expand_dims(hidden, [0])\n",
    "            step_size = tf.shape(self.imageIn)[:1]\n",
    "            state_in = tf.nn.rnn_cell.LSTMStateTuple(c_in, h_in)\n",
    "            lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "                    lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "                    time_major=False)\n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "            rnn_out = tf.reshape(lstm_outputs, [-1, 256])\n",
    "\n",
    "            # POLICY OUTPUT (Actor)\n",
    "            self.policy = tf.contrib.layers.fully_connected(inputs = rnn_out,\n",
    "                                                           num_outputs = action_size,\n",
    "                                                           activation_fn = tf.nn.softmax,\n",
    "                                                           weights_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                                           biases_initializer = None)\n",
    "\n",
    "            # VALUE OUTPUT (Critic)\n",
    "            self.value = tf.contrib.layers.fully_connected(inputs = rnn_out,\n",
    "                                                           num_outputs = 1,\n",
    "                                                           activation_fn = None,\n",
    "                                                           weights_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                                           biases_initializer = None)\n",
    "\n",
    "            # Only the worker network need operations for loss function and gradient updating.\n",
    "            if scope != 'global':\n",
    "                # Placeholders for the action, target value and advantages\n",
    "                self.action = tf.placeholder(tf.int32, [None], name=\"action\")\n",
    "                \n",
    "                # Compute the one hot vectors for each action given\n",
    "                action_one_hot = tf.one_hot(self.action, \n",
    "                                            action_size, \n",
    "                                            tf.float32, \n",
    "                                            name=\"action_one_hot\")\n",
    "                \n",
    "                # R\n",
    "                self.target_value = tf.placeholder(tf.float32, \n",
    "                                                   [None], \n",
    "                                                   name=\"target_value\")\n",
    "                \n",
    "                # A\n",
    "                self.advantages = tf.placeholder(tf.float32, \n",
    "                                                 [None], \n",
    "                                                 name=\"advantages\")\n",
    "                \n",
    "                \n",
    "                ### LOSS PART\n",
    "                # Clip the policy output to avoid 0 and 1 (bad for log calculation)\n",
    "                # tf.clip_by_value: given tensor f, this operation returns a tensor of the same type\n",
    "                # and shape as t with its values clipped to clip_value_min and clip_value_max. \n",
    "                # Any values less than clip_value_min are set to clip_value_min.Any values greater \n",
    "                # than clip_value_max are set to clip_value_max.\n",
    "                \n",
    "                # log(pi)\n",
    "                self.log_policy = tf.log(tf.clip_by_value(self.policy, 0.000001, 0.999999))\n",
    "                \n",
    "                # For a given state and action, compute the log of the policy at\n",
    "                # that action for that state.\n",
    "                # log(pi(s))\n",
    "                                                            # Tensor to reduce              # Dimension to reduce\n",
    "                self.log_pi_for_action = tf.reduce_sum(self.log_policy * self.actions_onehot, reduction_indices = 1)\n",
    "                \n",
    "                # VALUE LOSS:\n",
    "                # Squared difference between Expected discounted reward (R) and Value (V(s))\n",
    "                # L = Σ(R - V(s))²\n",
    "                # self.target_value - self.value\n",
    "                \n",
    "                self.value_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
    "                \n",
    "                \n",
    "                # POLICY LOSS:\n",
    "                # We want to do gradient ascent on the expected discounted reward\n",
    "                # (Because we want to max improve the expected discounted reward)\n",
    "                # Grad of R == grad logpi * (R - estimated V)\n",
    "                # Where R: sampled reward from given state following the policy pi\n",
    "                # self.log_pi_for_action * self.advantages\n",
    "                # Because we want to max this, we define the policy loss as the negative\n",
    "                # and get tf to do the automatic differentiation for us\n",
    "                self.policy_loss = -tf.reduce_mean(self.log_pi_for_action * self.advantages)\n",
    "                \n",
    "                \n",
    "                # Also add entropy as another loss to the policy\n",
    "                # Entropy of proba dist: is the expected value of logP(X)\n",
    "                # denonted E(-log P(X)), which can compute for our policy at any given\n",
    "                # state with Σ(policy * -log(policy)).\n",
    "                # This will be a number between 0 and 1.\n",
    "                #  Note that entropy is smaller when the probability\n",
    "                # distribution is more concentrated on one action, so a larger\n",
    "                # entropy implies more exploration. Thus we penalise small entropy,\n",
    "                # or equivalently, add -entropy to our loss. \n",
    "                self.entropy = tf.reduce_sum(tf.multiply(self.policy, -self.log_policy))\n",
    "                \n",
    "                \n",
    "                # Try to minimize the loss\n",
    "                # Note the negative entropy term, which encourages exploration:\n",
    "                # higher entropy corresponds to less certainty.\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "                \n",
    "                \n",
    "                # GRADIENTS\n",
    "                #A worker then uses these losses to obtain gradients with respect to its network \n",
    "                # parameters. Each of these gradients are typically clipped in order to prevent \n",
    "                # overly-large parameter updates which can destabilize the policy.\n",
    "                \n",
    "                # Compute the gradient of the loss with respect to all the weights,\n",
    "                # and create a list of tuples consisting of the gradient to apply to\n",
    "                # the weight.\n",
    "                # Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss,local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)\n",
    "\n",
    "                #Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each worker then interacts with its own copy of the environment and collects experience. Each keeps a list of experience tuples (observation, action, reward, done, value) that is constantly added to from interactions with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self, game, name, state_size, action_size, trainer, model_path,global_episodes):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name\n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        # Keep the global training\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        \n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        \n",
    "        # Tensorboard\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.number))\n",
    "\n",
    "        # Create a copy of the network and the tensorflow operators\n",
    "        # to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(state_size, action_size, self.name, trainer)\n",
    "        self.update_local_ops = update_target_graph('global', self.name)\n",
    "        \n",
    "        # Config doom\n",
    "        self.actions, self.env = config_doom(action_size)\n",
    "        \n",
    "    # TRAINING\n",
    "    def train(self, rollout, sess, gamma, bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        observations = rollout[:,0]\n",
    "        actions = rollout[:,1]\n",
    "        rewards = rollout[:,2]\n",
    "        next_observations = rollout[:,3]\n",
    "        values = rollout[:,5]\n",
    "        \n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. \n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.inputs:np.vstack(observations),\n",
    "            self.local_AC.actions:actions,\n",
    "            self.local_AC.advantages:advantages,\n",
    "            self.local_AC.state_in[0]:self.batch_rnn_state[0],\n",
    "            self.local_AC.state_in[1]:self.batch_rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n, self.batch_rnn_state,_ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.state_out,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n,v_n\n",
    "        \n",
    "    \n",
    "    # PLAYING\n",
    "    def work(self, max_episode_length, gamma, sess, coordinator, saver):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print(\"Starting worker \" + str(self.number))\n",
    "        \n",
    "         with sess.as_default(), sess.graph.as_default():\n",
    "            while not coordinator.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                done = False\n",
    "                \n",
    "                # Create a new episode\n",
    "                self.env.new_episode()\n",
    "                \n",
    "                # State\n",
    "                state = self.env.get_state().screen_buffer\n",
    "                \n",
    "                # Append that state to the buffer\n",
    "                episode_frames.append(state)\n",
    "                \n",
    "                # Preprocess the state\n",
    "                state = preprocess_frame(state, False)\n",
    "                \n",
    "                # ?\n",
    "                rnn_state = self.local_AC.state_init\n",
    "                \n",
    "                # ?\n",
    "                self.batch_rnn_state = rnn_state\n",
    "                \n",
    "                # While the episode is not finished\n",
    "                while self.env.is_episode_finished() == False:\n",
    "                    # Take an action using probabilities from policy network output\n",
    "                    action_distribution, value, rnn_state = sess.run([self.local_AC.policy, self.local_AC.value, self.local_AC.state_out],\n",
    "                                                                    feed_dict = {self.local_AC.inputs:[state],\n",
    "                                                                                self.local_AC.state_in[0]:rnn_state[0],\n",
    "                                                                                self.local_AC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "                    \n",
    "                    reward = self.env.make_action(self.actions[a]) / 100.0\n",
    "                    \n",
    "                    done = self.env.is_episode_finished()\n",
    "                    \n",
    "                    # If not done\n",
    "                    if done == False:\n",
    "                        # Get the new state\n",
    "                        new_state = self.env.get_state().screen_buffer\n",
    "                        episode_frames.append(new_state)\n",
    "                        new_state = preprocess_frame(new_state, False)\n",
    "                        \n",
    "                    else:\n",
    "                        new_state = state\n",
    "                    \n",
    "                    episode_buffer.append([state, action, reward, new_state, done, value[0,0]])\n",
    "                    episode_values.append(v[0,0])\n",
    "                    \n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    state = new_state\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                    \n",
    "                    #Specific to VizDoom. We sleep the game for a specific time.\n",
    "                    if self.sleep_time>0:\n",
    "                        sleep(self.sleep_time)\n",
    "                        \n",
    "                    # If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    # make an update step using that experience rollout.\n",
    "                    if len(episode_buffer) == 30 and d != True and episode_step_count != max_episode_length - 1:\n",
    "                        # Since we don't know what the true final return is, we \"bootstrap\" from our current\n",
    "                        # value estimation.\n",
    "                        v1 = sess.run(self.local_AC.value, \n",
    "                            feed_dict={self.local_AC.inputs:[s],\n",
    "                            self.local_AC.state_in[0]:rnn_state[0],\n",
    "                            self.local_AC.state_in[1]:rnn_state[1]})[0,0]\n",
    "                        v_l,p_l,e_l,g_n,v_n = self.train(global_AC,episode_buffer,sess,gamma,v1)\n",
    "                        episode_buffer = []\n",
    "                        sess.run(self.update_local_ops)\n",
    "                    if d == True:\n",
    "                        break\n",
    "                                            \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the episode buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "                                \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 5 == 0 and episode_count != 0:\n",
    "                    if self.name == 'worker_0' and episode_count % 25 == 0:\n",
    "                        time_per_step = 0.05\n",
    "                        images = np.array(episode_frames)\n",
    "                        make_gif(images,'./frames/image'+str(episode_count)+'.gif',\n",
    "                            duration=len(images)*time_per_step,true_image=True,salience=False)\n",
    "                    if episode_count % 250 == 0 and self.name == 'worker_0':\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print (\"Saved Model\")\n",
    "\n",
    "                    mean_reward = np.mean(self.episode_rewards[-5:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-5:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-5:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                    summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                    summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                    summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                    summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists('./frames'):\n",
    "    os.makedirs('./frames')\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "   \n",
    "    # Generate global network\n",
    "    master_network = AC_Network(state_size, action_size, 'global', None)\n",
    "    \n",
    "    # Set workers based on available CPU threads\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    workers = []\n",
    "    \n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(DoomGame(),\n",
    "                             i,\n",
    "                             state_size,\n",
    "                             action_size,\n",
    "                             trainer,\n",
    "                             saver,\n",
    "                             model_path))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "        \n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    # This class implements a simple mechanism to coordinate the termination of a set of threads.\n",
    "    coordinator = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print(\"Loading Model\")\n",
    "        checkpoint = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        \n",
    "    else:\n",
    "        init = tf.global_variables_initializer\n",
    "        sess.run(init)\n",
    "        \n",
    "    \n",
    "    # Asynchronous part\n",
    "    # Start the \"work\" process for each worker in a separate thread.\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: woker.work(max_episode_length,\n",
    "                                        gamma,\n",
    "                                        master_network,\n",
    "                                        sess,\n",
    "                                        coordinator)\n",
    "        \n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        \n",
    "        t.start()\n",
    "        \n",
    "        worker_threads.append(t)\n",
    "    \n",
    "    coordinator.join(worker_threads)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of worker agents, each with their own network and environment are created. Each of these workers are run on a separate processor thread, so there should be no more worker than there are threads on my CPU"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aind-dog]",
   "language": "python",
   "name": "conda-env-aind-dog-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
